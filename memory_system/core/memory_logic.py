import pandas as pd
import numpy as np
import os
from typing import List, Dict

# å‡è®¾ SemanticModel å¯ä»¥åœ¨åŒçº§ç›®å½•æˆ–çˆ¶çº§è·¯å¾„ä¸­å¯¼å…¥
from .semantic_model import SemanticModel
import Levenshtein
from tqdm import tqdm

# å¯¼å…¥ Jaro-Winkler ç›¸ä¼¼åº¦å‡½æ•°ï¼ˆå‡è®¾æ‚¨å·²å®‰è£… python-Levenshtein åº“ï¼‰
from Levenshtein import jaro_winkler
import math


# --- æ–‡ä»¶è·¯å¾„å®šä¹‰ ---
# åŸºäºå½“å‰æ–‡ä»¶ (memory_logic.py) çš„ç›¸å¯¹è·¯å¾„å®šä¹‰
# ğŸŒŸ ä¿®å¤/ç¡®è®¤ï¼šè·¯å¾„ä½¿ç”¨æ–°çš„ CET6_word_list.csv
DEFAULT_CSV_FILE = os.path.join(
    os.path.dirname(__file__), "..", "data", "CET6_word_list.csv"
)
DEFAULT_CACHE_FILE = os.path.join(
    os.path.dirname(__file__), "..", "data", "word_vectors_cache.npy"
)
# è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µç¼“å­˜æ–‡ä»¶è·¯å¾„
DEFAULT_SIMILARITY_MATRIX_FILE = os.path.join(
    os.path.dirname(__file__), "..", "data", "semantic_similarity_matrix.npy"
)
# --- æ–‡ä»¶è·¯å¾„å®šä¹‰ ---


class MemoryLogic:
    """
    è´Ÿè´£åŠ è½½æ•°æ®ã€å®ç°å­—å½¢ç›¸ä¼¼åº¦ã€èåˆè¯„åˆ†å’Œç”Ÿæˆè”æƒ³æ¨èã€‚
    å·²ä¼˜åŒ–ï¼šå¼•å…¥è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µé¢„è®¡ç®—ï¼Œæé«˜è”æƒ³æ¨èçš„å®æ—¶å“åº”é€Ÿåº¦ã€‚
    """

    def __init__(
        self,
        csv_file: str = DEFAULT_CSV_FILE,
        cache_file: str = DEFAULT_CACHE_FILE,
        matrix_file: str = DEFAULT_SIMILARITY_MATRIX_FILE,  # æ–°å¢å‚æ•°
        alpha: float = 0.6,
        beta: float = 0.4,
        top_n: int = 10,
    ):
        # å°†é…ç½®å‚æ•°ä¿å­˜ä¸ºå®ä¾‹å±æ€§
        self.csv_file = csv_file
        self.cache_file = cache_file
        self.matrix_file = matrix_file  # ä¿å­˜çŸ©é˜µæ–‡ä»¶è·¯å¾„
        self.ALPHA = alpha
        self.BETA = beta
        self.TOP_N = top_n

        # 1. åŠ è½½æ•°æ®
        self.word_data = self._load_data()
        self.words = self.word_data["word"].tolist()
        # åˆ›å»º word åˆ°å…¶åœ¨ self.words åˆ—è¡¨ä¸­ç´¢å¼•çš„æ˜ å°„
        self.word_to_index = {word: i for i, word in enumerate(self.words)}
        self.word_data.set_index(
            "word", inplace=True
        )  # ä¼˜åŒ–ï¼šè®¾ç½® word ä¸ºç´¢å¼•ï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾å«ä¹‰

        # 2. åˆå§‹åŒ–è¯­ä¹‰æ¨¡å‹ (BERT)
        # ğŸŒŸ å‡è®¾ SemanticModel èƒ½å¤Ÿè¢«æ­£ç¡®å¯¼å…¥ï¼Œå¦åˆ™è¿™é‡Œä¼šå¤±è´¥
        self.semantic_model = SemanticModel()

        # 3. ç¼“å­˜æ‰€æœ‰å•è¯çš„å‘é‡
        print("æ­£åœ¨è®¡ç®—/åŠ è½½æ‰€æœ‰å•è¯çš„ BERT å‘é‡ (åˆ©ç”¨ GPU)...")
        self.word_vectors = self._get_all_word_vectors()
        print("æ‰€æœ‰å•è¯å‘é‡ç¼“å­˜å®Œæˆã€‚")

        # 4. ğŸŒŸ æ–°å¢ï¼šè®¡ç®—/åŠ è½½è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ
        print("æ­£åœ¨è®¡ç®—/åŠ è½½è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ (ç”¨äºå¿«é€ŸæŸ¥è¯¢)...")
        self.semantic_matrix = self._get_all_semantic_similarity()
        print("è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µåŠ è½½å®Œæˆã€‚")

        # 5. åˆ›å»ºå­—å½¢/åŒæ ¹è¯ç´¢å¼•
        self.rel_words_index = self._create_rel_words_index()

    def _load_data(self) -> pd.DataFrame:
        """ä» CSV æ–‡ä»¶åŠ è½½å•è¯æ•°æ®ï¼Œå¹¶è¿›è¡ŒåŸºæœ¬é¢„å¤„ç†ã€‚"""
        if not os.path.exists(self.csv_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°è¯åº“æ–‡ä»¶: {self.csv_file}")

        # keep_default_na=False ç¡®ä¿ç©ºå€¼ï¼ˆå¦‚ rel_words å­—æ®µï¼‰è¢«è§†ä¸º '' è€Œä¸æ˜¯ NaN
        word_data = pd.read_csv(self.csv_file, keep_default_na=False)
        word_data["word"] = word_data["word"].str.lower()

        # ğŸŒŸ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å¿…éœ€çš„åˆ—å­˜åœ¨æˆ–è¢«å¡«å……ï¼Œä»¥é˜²éŸ³æ ‡å’Œè¯æ€§æŸ¥æ‰¾å¤±è´¥
        for col in ["pos", "meaning_zh", "phonetic_us", "phonetic_uk"]:
            if col not in word_data.columns:
                word_data[col] = ""  # å‡è®¾ç¼ºå¤±æ—¶ä¸ºç©ºå­—ç¬¦ä¸²
            else:
                word_data[col] = word_data[col].fillna("")

        return word_data

    def _get_all_word_vectors(self) -> Dict[str, np.ndarray]:
        """åŠ è½½æˆ–è®¡ç®—æ‰€æœ‰å•è¯çš„ BERT å‘é‡å¹¶ç¼“å­˜ã€‚"""
        # ... (æ­¤æ–¹æ³•é€»è¾‘ä¿æŒä¸å˜)
        if os.path.exists(self.cache_file):
            print("æ­£åœ¨ä»ç¼“å­˜åŠ è½½å•è¯å‘é‡...")
            try:
                # å¿…é¡»ä½¿ç”¨ allow_pickle=True æ¥åŠ è½½ Dict å¯¹è±¡
                cache = np.load(self.cache_file, allow_pickle=True).item()
                if set(cache.keys()) == set(self.words):
                    print("æˆåŠŸåŠ è½½ç¼“å­˜ã€‚")
                    return cache
                else:
                    print("âš ï¸ ç¼“å­˜ä¸å½“å‰è¯åº“ä¸åŒ¹é…ï¼Œå°†é‡æ–°è®¡ç®—ã€‚")
            except Exception:
                print("âš ï¸ ç¼“å­˜æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°è®¡ç®—ã€‚")
        else:
            print("æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œå°†è®¡ç®—æ‰€æœ‰å•è¯å‘é‡ã€‚")

        # é‡æ–°è®¡ç®—é€»è¾‘
        word_vectors = {}

        # ğŸŒŸ æ ¸å¿ƒä¿®æ”¹ 2: ä½¿ç”¨ tqdm åŒ…è£… self.words åˆ—è¡¨ï¼Œæ˜¾ç¤ºè®¡ç®—è¿›åº¦
        for word in tqdm(self.words, desc="è®¡ç®— BERT å‘é‡"):
            try:
                word_vectors[word] = self.semantic_model.get_word_embedding(word)
            except Exception:
                print(f"âŒ è­¦å‘Šï¼šæ— æ³•è·å–è¯æ±‡ '{word}' çš„å‘é‡ï¼Œè·³è¿‡ã€‚")
                continue

        # ä¿å­˜åˆ°ç¼“å­˜
        try:
            np.save(self.cache_file, word_vectors)
        except Exception:
            pass  # å¿½ç•¥ä¿å­˜å¤±è´¥

        return word_vectors

    def _get_all_semantic_similarity(self) -> np.ndarray:
        """
        ä¼˜åŒ–æ ¸å¿ƒï¼šè®¡ç®—æˆ–åŠ è½½æ‰€æœ‰å•è¯çš„è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ (N x N)ã€‚
        """
        # ... (æ­¤æ–¹æ³•é€»è¾‘ä¿æŒä¸å˜)
        N = len(self.words)

        # 1. å°è¯•ä»ç¼“å­˜åŠ è½½çŸ©é˜µ
        if os.path.exists(self.matrix_file):
            print("æ­£åœ¨ä»ç¼“å­˜åŠ è½½è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ...")
            try:
                matrix = np.load(self.matrix_file)
                if matrix.shape == (N, N):
                    print("æˆåŠŸåŠ è½½ç›¸ä¼¼åº¦çŸ©é˜µã€‚")
                    return matrix
                else:
                    print(
                        f"âš ï¸ ç¼“å­˜çŸ©é˜µå½¢çŠ¶ä¸åŒ¹é… ({matrix.shape} vs {(N, N)})ï¼Œå°†é‡æ–°è®¡ç®—ã€‚"
                    )
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜çŸ©é˜µåŠ è½½å¤±è´¥ ({e})ï¼Œå°†é‡æ–°è®¡ç®—ã€‚")
        else:
            print("æœªæ‰¾åˆ°ç›¸ä¼¼åº¦çŸ©é˜µç¼“å­˜ï¼Œå¼€å§‹è®¡ç®—ã€‚")

        # 2. é‡æ–°è®¡ç®—çŸ©é˜µ
        matrix = np.zeros((N, N), dtype=np.float32)

        # æå–æ‰€æœ‰å‘é‡å¹¶å †å æˆä¸€ä¸ª NumPy æ•°ç»„ (æé«˜æ•ˆç‡)
        # ä¿®å¤ï¼šç¡®ä¿ only words with vectors are included
        words_with_vectors = [word for word in self.words if word in self.word_vectors]
        N_valid = len(words_with_vectors)

        if N_valid != N:
            print(f"âŒ è­¦å‘Šï¼š{N - N_valid} ä¸ªè¯æ±‡å‘é‡ç¼ºå¤±ï¼Œå°†ä½¿ç”¨éƒ¨åˆ†çŸ©é˜µã€‚")

        # ä¼˜åŒ–ï¼šåªéœ€è®¡ç®—ä¸Šä¸‰è§’çŸ©é˜µ
        total_steps = N * (N - 1) // 2
        pbar = tqdm(total=total_steps, desc="è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ")

        for i in range(N):
            vec_i = self.word_vectors.get(self.words[i])
            if vec_i is None:
                # è¡¥å¿è·³è¿‡çš„è¿›åº¦æ­¥æ•°ï¼šç”±äº i çš„å¾ªç¯å·²ç»å®Œæˆï¼Œæˆ‘ä»¬è·³è¿‡ j > i çš„éƒ¨åˆ†
                pbar.update(N - 1 - i)
                continue

            # ä» i å¼€å§‹ï¼Œè®¡ç®—ä¸è‡ªå·±çš„ç›¸ä¼¼åº¦ (1.0) å’Œä¸å…¶ä»–è¯çš„ç›¸ä¼¼åº¦
            for j in range(i, N):
                vec_j = self.word_vectors.get(self.words[j])
                if vec_j is None:
                    if j > i:  # åªæœ‰å½“ j > i æ—¶ï¼Œæ‰éœ€è¦æ›´æ–° pbar
                        pbar.update(1)
                    continue

                # ğŸŒŸ æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ semantic_model.calculate_semantic_similarity æ¥å— NumPy æ•°ç»„
                sim = self.semantic_model.calculate_semantic_similarity(vec_i, vec_j)
                matrix[i, j] = sim
                matrix[j, i] = sim  # å¯¹ç§°å¡«å……

                if j > i:
                    pbar.update(1)  # åªåœ¨è®¡ç®—éå¯¹è§’çº¿å…ƒç´ æ—¶æ›´æ–°è¿›åº¦æ¡

        pbar.close()

        # 3. ä¿å­˜åˆ°ç¼“å­˜
        try:
            np.save(self.matrix_file, matrix)
        except Exception:
            print("âŒ è­¦å‘Šï¼šç›¸ä¼¼åº¦çŸ©é˜µä¿å­˜å¤±è´¥ã€‚")

        return matrix

    def _create_rel_words_index(self) -> Dict[str, str]:
        # ... (æ­¤æ–¹æ³•é€»è¾‘ä¿æŒä¸å˜)
        """åˆ›å»ºæ‰€æœ‰åŒæ ¹è¯(rel_words)åˆ°ä¸»è¯(word)çš„æ˜ å°„ç´¢å¼•ã€‚"""
        rel_words_index = {}
        for main_word, row in self.word_data.iterrows():  # ğŸŒŸ ä¼˜åŒ–ï¼šä½¿ç”¨ iterrows éå†
            rel_words_str = str(row["rel_words"])

            rel_words_index[main_word] = main_word

            if rel_words_str:
                for rel_word in rel_words_str.split("|"):
                    rel_word_stripped = rel_word.strip().lower()
                    if rel_word_stripped and rel_word_stripped not in rel_words_index:
                        rel_words_index[rel_word_stripped] = main_word

        return rel_words_index

    def calculate_S_form(self, word1: str, word2: str) -> float:
        # ... (æ­¤æ–¹æ³•é€»è¾‘ä¿æŒä¸å˜)
        """
        ä¼˜åŒ–ï¼šè®¡ç®—ä¸¤ä¸ªå•è¯çš„å­—å½¢ç›¸ä¼¼åº¦ S_Form (0.0 åˆ° 1.0)ã€‚
        èåˆäº†åŒæ ¹è¯æ£€æŸ¥ã€Jaro-Winkler å’Œ Levenshtein ç›¸ä¼¼åº¦ï¼Œä»¥æé«˜å‡†ç¡®æ€§ã€‚
        """

        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºåŒæ ¹è¯ (æ¥è‡ª CSV é¢„è®¾)
        root1 = self.rel_words_index.get(word1)
        root2 = self.rel_words_index.get(word2)

        if root1 and root1 == root2:
            return 1.0  # é¢„è®¾çš„åŒæ ¹è¯ç»™äºˆæ»¡åˆ†

        # 2. å¦‚æœä¸æ˜¯åŒæ ¹è¯ï¼Œè®¡ç®—åŸºäºå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦
        max_len = max(len(word1), len(word2))

        if max_len == 0:
            return 0.0

        # Levenshtein ç›¸ä¼¼åº¦
        distance = Levenshtein.distance(word1, word2)
        sim_lev = 1.0 - (distance / max_len)

        # Jaro-Winkler ç›¸ä¼¼åº¦ (é€šå¸¸åœ¨åå­—å’Œå•è¯åŒ¹é…ä¸Šè¡¨ç°æ›´å¥½)
        sim_jaro_winkler = jaro_winkler(word1, word2)

        # èåˆä¸¤ç§ç›¸ä¼¼åº¦ï¼Œå¹¶ç»™äºˆæ›´é«˜çš„æƒé‡ç»™ Jaro-Winkler
        similarity = 0.6 * sim_jaro_winkler + 0.4 * sim_lev

        return max(0.0, similarity)

    def _get_full_display_word(self, word: str, word_info: pd.Series) -> str:
        """ä» Series ä¸­è·å–éŸ³æ ‡ï¼Œç»„åˆæˆ 'word [phonetic]' å½¢å¼ã€‚"""
        # å°è¯•è·å–éŸ³æ ‡
        phonetic = ""
        try:
            # ä¼˜å…ˆä½¿ç”¨ç¾å¼éŸ³æ ‡
            if word_info.get("phonetic_us"):
                phonetic = word_info["phonetic_us"]
            # å…¶æ¬¡ä½¿ç”¨è‹±å¼éŸ³æ ‡
            elif word_info.get("phonetic_uk"):
                phonetic = word_info["phonetic_uk"]
        except KeyError:
            # å¦‚æœéŸ³æ ‡åˆ—ä¸å­˜åœ¨ï¼Œåˆ™è·³è¿‡
            pass

        # ç»„åˆå•è¯å’ŒéŸ³æ ‡ (ä¾‹å¦‚: "word [phonetic]")
        if phonetic:
            # å»é™¤éŸ³æ ‡å­—ç¬¦ä¸²å¯èƒ½å­˜åœ¨çš„é¢å¤–æ‹¬å·ï¼Œç„¶åç”¨æ–¹æ‹¬å·åŒ…å›´
            phonetic_clean = phonetic.strip().strip("[]")
            return f"{word} [{phonetic_clean}]"
        else:
            return word

    def get_word_meaning_by_word(self, word: str) -> str:
        """ä»åŠ è½½çš„æ•°æ®ä¸­å¿«é€ŸæŸ¥æ‰¾å•è¯çš„ä¸­æ–‡å«ä¹‰ï¼Œå¹¶é¢„ç½®è¯æ€§ã€‚"""
        try:
            # 1. æŸ¥æ‰¾å¯¹åº”çš„è¡Œæ•°æ®
            word_info = self.word_data.loc[word.lower()]
            # 2. æå–è¯æ€§å’Œå«ä¹‰
            pos = word_info["pos"]
            meaning_zh = word_info["meaning_zh"]
            # 3. ç»„åˆå¹¶è¿”å› (e.g., "n. åè¯å«ä¹‰")
            return f"{pos}. {meaning_zh}"
        except KeyError:
            return "ï¼ˆå«ä¹‰ç¼ºå¤±ï¼‰"

    # ğŸŒŸ æ–°å¢æ–¹æ³•ï¼šè·å–å®Œæ•´çš„æ˜¾ç¤ºå•è¯ï¼ˆå«éŸ³æ ‡ï¼‰
    def get_full_display_word(self, word: str) -> str:
        """è·å–å¸¦éŸ³æ ‡çš„å®Œæ•´æ˜¾ç¤ºå•è¯å­—ç¬¦ä¸²ã€‚"""
        try:
            word_info = self.word_data.loc[word.lower()]
            return self._get_full_display_word(word.lower(), word_info)
        except KeyError:
            return word  # æ‰¾ä¸åˆ°åˆ™è¿”å›åŸå§‹å•è¯

    def get_association_recommendations(
        self,
        target_word: str,
        mode: str = "total",  # æ¥å—æ–°çš„æ¨¡å¼å‚æ•°
        threshold: float = 0.4,  # è°ƒæ•´é»˜è®¤é˜ˆå€¼ï¼šé˜²æ­¢è¿‡æ»¤æ‰æ‰€æœ‰ç»“æœ
        top_n: int | None = None,  # ä¿®å¤ï¼šæ·»åŠ  top_n å‚æ•°ï¼Œå…è®¸è¿è¡Œæ—¶è¦†ç›– self.TOP_N
    ) -> List[Dict]:
        """
        ä¼˜åŒ–æ ¸å¿ƒï¼šä½¿ç”¨é¢„è®¡ç®—çš„è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µï¼Œå°† O(N) å®æ—¶è®¡ç®—é™ä¸º O(1) æŸ¥æ‰¾ã€‚
        ä¸ºç›®æ ‡è¯è®¡ç®—å¹¶è¿”å›è”æƒ³è¯æ¨èåˆ—è¡¨ï¼Œæ”¯æŒæŒ‰æ¨¡å¼å’Œé˜ˆå€¼è¿‡æ»¤ã€‚
        """

        # ç¡®å®šæœ€ç»ˆè¦è¿”å›çš„æ•°é‡ï¼Œå¦‚æœ top_n æœªæä¾›ï¼Œåˆ™ä½¿ç”¨å®ä¾‹å±æ€§
        final_top_n = top_n if top_n is not None else self.TOP_N

        target_word_lower = target_word.lower()
        if target_word_lower not in self.word_to_index:
            return []

        # 1. ç¡®å®šç›®æ ‡è¯çš„ç´¢å¼•
        target_index = self.word_to_index[target_word_lower]

        # 2. ä»é¢„è®¡ç®—çŸ©é˜µä¸­å–å‡ºç›®æ ‡è¯çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆO(1) æ“ä½œï¼‰
        # S_sem_vector æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰è¯æ±‡è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°çš„ N ç»´å‘é‡
        S_sem_vector = self.semantic_matrix[target_index, :]

        recommendations = []

        # 3. éå†æ‰€æœ‰å€™é€‰è¯ (O(N) å¾ªç¯ï¼Œä½†ç§»é™¤äº†è€—æ—¶çš„ BERT è®¡ç®—)
        for i, candidate_word in enumerate(self.words):
            candidate_word_lower = candidate_word

            if target_index == i:
                continue

            try:
                # å®æ—¶æŸ¥è¯¢ S_è¯­ä¹‰ï¼šç›´æ¥ä»çŸ©é˜µä¸­è·å– (O(1))
                S_sem = S_sem_vector[i]

                # å®æ—¶è®¡ç®— S_å­—å½¢ï¼šä»ç„¶éœ€è¦å®æ—¶è®¡ç®—ï¼Œä½†æ¯” BERT å¿«å¾—å¤š
                S_form = self.calculate_S_form(target_word_lower, candidate_word_lower)

                # 4. æ ¹æ® mode ç¡®å®šæœ€ç»ˆè¯„åˆ†å’Œè¿‡æ»¤å€¼
                score = 0.0
                should_keep = True

                S_total_actual = self.ALPHA * S_sem + self.BETA * S_form

                if mode == "total":
                    score = S_total_actual
                elif mode == "semantic":
                    score = S_sem
                elif mode == "form":
                    score = S_form
                else:
                    should_keep = False  # é‡åˆ°æœªçŸ¥æ¨¡å¼ï¼Œè·³è¿‡

                # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
                if score < threshold:
                    should_keep = False

                if not should_keep:
                    continue

                # 5. è®°å½•ç»“æœ
                recommendations.append(
                    {
                        "word": candidate_word,
                        # ğŸŒŸ ä¿®å¤: å¼ºåˆ¶è½¬æ¢ä¸º Python åŸç”Ÿ floatï¼Œç¡®ä¿ JSON åºåˆ—åŒ–æˆåŠŸ
                        "S_total": float(S_total_actual),
                        "S_semantic": float(S_sem),
                        "S_form": float(S_form),
                        "meaning_zh": self.get_word_meaning_by_word(candidate_word),
                        "primary_score": float(score),  # æ­¤æ¬¡æ’åº/è¿‡æ»¤çš„ä¾æ®
                    }
                )

            except Exception as e:
                # print(f"é”™è¯¯å¤„ç†è¯æ±‡ {candidate_word}: {e}")
                continue

        # 6. æ’åºï¼šæŒ‰ primary_score é™åºæ’åº
        recommendations.sort(key=lambda x: x["primary_score"], reverse=True)

        # 7. è¿”å›ç»“æœï¼šè¿”å› final_top_n
        return recommendations[:final_top_n]
